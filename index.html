<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>On Evaluating Adversarial Robustness of Large Vision-Language Models</title>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;800&display=swap" rel="stylesheet">
<style>
body {
  font-family: 'Inter', sans-serif;
  margin: 40px auto;
  max-width: 900px;
  line-height: 1.6;
  color: #333;
  text-align: center;
}
h1 { font-size: 2.2em; font-weight: 800; margin-bottom: 0.3em; }
h2 { font-size: 1.4em; margin-top: 2em; }
a { color: #007acc; text-decoration: none; }
a:hover { text-decoration: underline; }
.authors { font-size: 1.1em; margin-bottom: 0.5em; }
.affiliations { color: #666; font-size: 0.95em; margin-bottom: 1.5em; }
.buttons { margin: 20px 0; }
button {
  background: #222; color: white; border: none; padding: 10px 18px;
  border-radius: 25px; margin: 5px; cursor: pointer; font-weight: 600;
}
button:hover { background: #444; }
.abstract { text-align: justify; margin-top: 2em; }
</style>
</head>

<body>

<h1>On Evaluating Adversarial Robustness of Large Vision-Language Models</h1>

<p class="authors">
  <a href="#">Yunqing Zhao</a><sup>1*</sup>,
  <a href="#">Tianyu Pang</a><sup>2â€ </sup>,
  <a href="#">Chao Du</a><sup>2â€ </sup>,
  <a href="#">Xiao Yang</a><sup>3</sup>,
  <a href="#">Chongxuan Li</a><sup>4</sup>,
  <a href="#">Ngai-Man Cheung</a><sup>1â€ </sup>,
  <a href="#">Min Lin</a><sup>2</sup>
</p>

<p class="affiliations">
  <sup>1</sup>Singapore University of Technology and Design, 
  <sup>2</sup>Tsinghua University, 
  <sup>3</sup>Renmin University of China, 
  <sup>4</sup>Sea AI Lab, Singapore
</p>

<div class="buttons">
  <button onclick="window.open('https://arxiv.org/abs/xxxxxx')">ðŸ“„ Paper</button>
  <button onclick="window.open('https://github.com/yourname/YourProjectName')">ðŸ’» Code</button>
  <button onclick="window.open('https://drive.google.com/...')">ðŸ“Š Data</button>
</div>

<h2>Abstract</h2>
<p class="abstract">
Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs. However, multimodal integration exacerbates safety concerns, as adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision).
</p>

</body>
</html>
